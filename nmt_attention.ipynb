{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshmichaitanyach/Project1_SPEC/blob/master/nmt_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEaHrrP7G7rH",
        "colab_type": "code",
        "outputId": "ed22fbf9-5386-45a9-91c0-014ee1666704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "# Download the file\n",
        "#path_to_zip = tf.keras.utils.get_file(\n",
        " #   'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "  #  extract=True)\n",
        "\n",
        "#path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
        "#lines= pd.read_table('/content/fra.txt', names=['eng', 'fra'], encoding='utf-8', header=None, usecols=[0,1])\n",
        "#print(lines)\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "X = '/content/fra.txt'\n",
        "new_doc = load_doc(X)\n",
        "\n",
        "with io.open(X, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read()\n",
        "#lines = lines.decode('utf-8')\n",
        "#print(lines[:5])\n",
        "word_pairs = []\n",
        "for line in lines.split('\\n'):\n",
        "  word_pairs.append(line.split('\\t')[0:2])\n",
        "print(\"these are\")\n",
        "print(word_pairs[:6])\n",
        "#word_pairs_new = word_pairs[:0]\n",
        "#print(\"this one\")\n",
        "#print(word_pairs_new[])\n",
        "#word_pairs_old = word_pairs[1:]\n",
        "word_pairs = word_pairs[:30000]\n",
        "len(word_pairs)\n",
        "#print(word_pairs[1:])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "these are\n",
            "[['Go.', 'Va !'], ['Hi.', 'Salut !'], ['Hi.', 'Salut.'], ['Run!', 'Cours\\u202f!'], ['Run!', 'Courez\\u202f!'], ['Who?', 'Qui ?']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA_16XeqOkvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMWw74eWHEpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "  s = unicode_to_ascii(s)\n",
        "  s = re.sub(r'([!.?])', r' \\1', s)\n",
        "  s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "  s = re.sub(r'\\s+', r' ', s)\n",
        " \n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2vYLaQWHOgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "#def create_dataset(path, num_examples):\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  #lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  #word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        " # return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEdSkxkvHQlm",
        "colab_type": "code",
        "outputId": "9f3a0197-6455-4eec-b615-09750ccce4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "en, sp = list(zip(*word_pairs))\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Make another choice.\n",
            "Faites un autre choix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qihU-z3sHT9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk4cbnjEHZF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = list(create_dataset(path, num_examples))\n",
        "\n",
        "  #input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  #target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return targ_lang, inp_lang"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14wttRpCGGSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = 30000//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "RNN_SIZE = 512\n",
        "\n",
        "ATTENTION_FUNC = 'concat'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb3EkbngHbhR",
        "colab_type": "code",
        "outputId": "a49a4805-72e4-410e-9eaa-c50ab1a15729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "raw_data_en, raw_data_fr = list(zip(*word_pairs))\n",
        "\n",
        "raw_data_en = [preprocess_sentence(data) for data in raw_data_en]\n",
        "raw_data_fr_in = ['<start> ' + preprocess_sentence(data) for data in raw_data_fr]\n",
        "raw_data_fr_out = [preprocess_sentence(data) + ' <end>' for data in raw_data_fr]\n",
        "\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
        "                                                        padding='post')\n",
        "print('English sequences')\n",
        "print(data_en[:2])\n",
        "\n",
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_out)\n",
        "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
        "                                                           padding='post')\n",
        "print('French input sequences')\n",
        "print(data_fr_in[:2])\n",
        "\n",
        "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,\n",
        "                                                            padding='post')\n",
        "print('French output sequences')\n",
        "print(data_fr_out[:2])\n",
        "\n",
        "\n",
        "vocab_inp_size = len(en_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(fr_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_en, data_fr_in, data_fr_out))\n",
        "dataset = dataset.shuffle(len(raw_data_en)).batch(\n",
        "    BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "#max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English sequences\n",
            "[[  30    1    0    0    0    0    0    0]\n",
            " [1156    1    0    0    0    0    0    0]]\n",
            "French input sequences\n",
            "[[   2   88   12    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   2 1091   12    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "French output sequences\n",
            "[[  88   12    3    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1091   12    3    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMtj6vrrHd7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "#input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "#print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTCoOd8wHnQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "#dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xZlAIayHpuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#example_input_batch, example_target_batch = next(iter(dataset))\n",
        "#example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJrx-OyMd1tD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, rnn_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.rnn_size = rnn_size\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            rnn_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, sequence, states):\n",
        "        embed = self.embedding(sequence)\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        return (tf.zeros([batch_size, self.rnn_size]),\n",
        "                tf.zeros([batch_size, self.rnn_size]))\n",
        "\n",
        "\n",
        "#en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, RNN_SIZE)\n",
        "\n",
        "\n",
        "class LuongAttention(tf.keras.Model):\n",
        "    def __init__(self, rnn_size, attention_func):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.attention_func = attention_func\n",
        "\n",
        "        if attention_func not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(\n",
        "                'Unknown attention score function! Must be either dot, general or concat.')\n",
        "\n",
        "        if attention_func == 'general':\n",
        "            # General score function\n",
        "            self.wa = tf.keras.layers.Dense(rnn_size)\n",
        "        elif attention_func == 'concat':\n",
        "            # Concat score function\n",
        "            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "            self.va = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, decoder_output, encoder_output):\n",
        "        if self.attention_func == 'dot':\n",
        "            # Dot score function: decoder_output (dot) encoder_output\n",
        "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "            # => score has shape: (batch_size, 1, max_len)\n",
        "            score = tf.matmul(decoder_output, encoder_output, transpose_b=True)\n",
        "        elif self.attention_func == 'general':\n",
        "            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
        "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "            # => score has shape: (batch_size, 1, max_len)\n",
        "            score = tf.matmul(decoder_output, self.wa(\n",
        "                encoder_output), transpose_b=True)\n",
        "        elif self.attention_func == 'concat':\n",
        "            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
        "            # Decoder output must be broadcasted to encoder output's shape first\n",
        "            decoder_output = tf.tile(\n",
        "                decoder_output, [1, encoder_output.shape[1], 1])\n",
        "\n",
        "            # Concat => Wa => va\n",
        "            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
        "            score = self.va(\n",
        "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1)))\n",
        "\n",
        "            # Transpose score vector to have the same shape as other two above\n",
        "            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
        "            score = tf.transpose(score, [0, 2, 1])\n",
        "\n",
        "        # alignment a_t = softmax(score)\n",
        "        alignment = tf.nn.softmax(score, axis=2)\n",
        "\n",
        "        # context vector c_t is the weighted average sum of encoder output\n",
        "        context = tf.matmul(alignment, encoder_output)\n",
        "\n",
        "        return context, alignment\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, rnn_size, attention_func):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention = LuongAttention(rnn_size, attention_func)\n",
        "        self.rnn_size = rnn_size\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            rnn_size, return_sequences=True, return_state=True)\n",
        "        self.wc = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "        self.ws = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, sequence, state, encoder_output):\n",
        "        # Remember that the input to the decoder\n",
        "        # is now a batch of one-word sequences,\n",
        "        # which means that its shape is (batch_size, 1)\n",
        "        embed = self.embedding(sequence)\n",
        "\n",
        "        # Therefore, the lstm_out has shape (batch_size, 1, rnn_size)\n",
        "        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
        "\n",
        "        # Use self.attention to compute the context and alignment vectors\n",
        "        # context vector's shape: (batch_size, 1, rnn_size)\n",
        "        # alignment vector's shape: (batch_size, 1, source_length)\n",
        "        context, alignment = self.attention(lstm_out, encoder_output)\n",
        "\n",
        "        # Combine the context vector and the LSTM output\n",
        "        # Before combined, both have shape of (batch_size, 1, rnn_size),\n",
        "        # so let's squeeze the axis 1 first\n",
        "        # After combined, it will have shape of (batch_size, 2 * rnn_size)\n",
        "        lstm_out = tf.concat(\n",
        "            [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
        "\n",
        "        # lstm_out now has shape (batch_size, rnn_size)\n",
        "        lstm_out = self.wc(lstm_out)\n",
        "\n",
        "        # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
        "        logits = self.ws(lstm_out)\n",
        "\n",
        "        return logits, state_h,state_c, alignment\n",
        "\n",
        "\n",
        "#fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, RNN_SIZE, ATTENTION_FUNC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-YXLom4PG8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "#loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        " #   from_logits=True, reduction='none')\n",
        "\n",
        "initial_state = encoder.init_states(1)\n",
        "encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
        "decoder_outputs = decoder(tf.constant(\n",
        "    [[1]]), encoder_outputs[1:], encoder_outputs[0])\n",
        "def loss_function(targets, logits):\n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True)\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "#def loss_function(real, pred):\n",
        " # mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  #loss_ = loss_object(real, pred)\n",
        "\n",
        "  #mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  #loss_ *= mask\n",
        "\n",
        "  #return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGmr8vzPPH8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0OXy9TgPKZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, train_seq_out, en_initial_states):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        en_outputs = encoder(source_seq, en_initial_states)\n",
        "        en_states = en_outputs[1:]\n",
        "        de_state_h, de_state_c = en_states\n",
        "\n",
        "        # We need to create a loop to iterate through the target sequences\n",
        "        for i in range(target_seq_out.shape[1]):\n",
        "            # Input to the decoder must have shape of (batch_size, length)\n",
        "            # so we need to expand one dimension\n",
        "            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
        "            logit, de_state_h, de_state_c, _ = decoder(\n",
        "                decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
        "\n",
        "            # The loss is now accumulated through the whole batch\n",
        "            loss += loss_function(target_seq_out[:, i], logit)\n",
        "            #loss = np.int32(loss)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return tf.cast(loss, tf.int32) / target_seq_out.shape[1]\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dgGz6XdiiSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "    if test_source_text is None:\n",
        "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_initial_states = encoder.init_states(1)\n",
        "    en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)\n",
        "\n",
        "    de_input = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
        "    de_state_h,de_state_c = en_outputs[1:]\n",
        "    out_words = []\n",
        "    alignments = []\n",
        "\n",
        "    while True:\n",
        "        de_output, de_state_h,de_state_c, alignment = decoder(\n",
        "            de_input, de_state_h, de_state_c, en_outputs[0])\n",
        "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
        "        out_words.append(fr_tokenizer.index_word[de_input.numpy()[0][0]])\n",
        "\n",
        "        alignments.append(alignment.numpy())\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))\n",
        "    return np.array(alignments), test_source_text.split(' '), out_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q2W2fo8PMPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "f9156247-2211-445d-8256-4fcefcc85fab"
      },
      "source": [
        "if not os.path.exists('checkpoints_luong/encoder'):\n",
        "    os.makedirs('checkpoints_luong/encoder')\n",
        "if not os.path.exists('checkpoints_luong/decoder'):\n",
        "    os.makedirs('checkpoints_luong/decoder')\n",
        "\n",
        "# Uncomment these lines for inference mode\n",
        "encoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/encoder')\n",
        "decoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/decoder')\n",
        "\n",
        "if encoder_checkpoint is not None and decoder_checkpoint is not None:\n",
        "    encoder.load_weights(encoder_checkpoint)\n",
        "    decoder.load_weights(decoder_checkpoint)\n",
        "EPOCHS = 3\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "        en_initial_states = encoder.init_states(BATCH_SIZE)\n",
        "        encoder.save_weights(\n",
        "            'checkpoints_luong/encoder/encoder_{}.h5'.format(e + 1))\n",
        "        decoder.save_weights(\n",
        "            'checkpoints_luong/decoder/decoder_{}.h5'.format(e + 1))\n",
        "        for (batch, (source_seq, target_seq_in, target_seq_out)) in enumerate(dataset.take(-1)):\n",
        "            loss = train_step(source_seq, target_seq_in, target_seq_out, en_initial_states)\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "                    e + 1, batch, loss.numpy()))\n",
        "\n",
        "        try:\n",
        "            predict()\n",
        "\n",
        "            predict(\"How are you today ?\")\n",
        "        except Exception:\n",
        "            continue\n",
        "        #if (epoch + 1) % 2 == 0:\n",
        "        #  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1 Batch 0 Loss 3.4375\n",
            "Epoch 1 Batch 100 Loss 1.3125\n",
            "Epoch 1 Batch 200 Loss 1.2500\n",
            "Epoch 1 Batch 300 Loss 1.3125\n",
            "Epoch 1 Batch 400 Loss 1.1250\n",
            "Crime is down .\n",
            "[[814, 8, 145, 1]]\n",
            "Epoch 2 Batch 0 Loss 1.1250\n",
            "Epoch 2 Batch 100 Loss 1.1250\n",
            "Epoch 2 Batch 200 Loss 1.1250\n",
            "Epoch 2 Batch 300 Loss 1.1250\n",
            "Epoch 2 Batch 400 Loss 1.1250\n",
            "I was studying .\n",
            "[[2, 21, 867, 1]]\n",
            "Epoch 3 Batch 0 Loss 1.1250\n",
            "Epoch 3 Batch 100 Loss 1.1250\n",
            "Epoch 3 Batch 200 Loss 1.1250\n",
            "Epoch 3 Batch 300 Loss 1.1250\n",
            "Epoch 3 Batch 400 Loss 1.1250\n",
            "I m sad without you .\n",
            "[[2, 13, 276, 1040, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjiMunQ3E_xk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "1aec4776-59a8-4c47-f0f6-7aff0c675f73"
      },
      "source": [
        "if not os.path.exists('heatmap'):\n",
        "    os.makedirs('heatmap')\n",
        "\n",
        "test_sents = (\n",
        "    'What a ridiculous concept!',\n",
        "    'Your idea is not entirely crazy.',\n",
        "    \"A man's worth lies in what he is.\",\n",
        "    'What he did is very wrong.',\n",
        "    \"All three of you need to do that.\",\n",
        "    \"Are you giving me another chance?\",\n",
        "    \"Both Tom and Mary work as models.\",\n",
        "    \"Can I have a few minutes, please?\",\n",
        "    \"Could you close the door, please?\",\n",
        "    \"Did you plant pumpkins this year?\",\n",
        "    \"Do you ever study in the library?\",\n",
        "    \"Don't be deceived by appearances.\",\n",
        "    \"Excuse me. Can you speak English?\",\n",
        "    \"Few people know the true meaning.\",\n",
        "    \"Germany produced many scientists.\",\n",
        "    \"Guess whose birthday it is today.\",\n",
        "    \"He acted like he owned the place.\",\n",
        "    \"Honesty will pay in the long run.\",\n",
        "    \"How do we know this isn't a trap?\",\n",
        "    \"I can't believe you're giving up.\",\n",
        ")\n",
        "\n",
        "filenames = []\n",
        "\n",
        "for i, test_sent in enumerate(test_sents):\n",
        "    test_sequence = preprocess_sentence(test_sent)\n",
        "    alignments, source, prediction = predict(test_sequence)\n",
        "    attention = np.squeeze(alignments, (1, 2))\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='jet')\n",
        "    ax.set_xticklabels([''] + source, rotation=90)\n",
        "    ax.set_yticklabels([''] + prediction)\n",
        "\n",
        "    filenames.append('heatmap/test_{}.png'.format(i))\n",
        "    plt.savefig('heatmap/test_{}.png'.format(i))\n",
        "    plt.close()\n",
        "\n",
        "with imageio.get_writer('translation_heatmaps.gif', mode='I', duration=2) as writer:\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What a ridiculous concept !\n",
            "[[36, 6, 1135, 31]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-457cff3d904f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0malignments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-a871b0b68774>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(test_source_text)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         de_output, de_state_h,de_state_c, alignment = decoder(\n\u001b[0;32m---> 18\u001b[0;31m             de_input, de_state_h, de_state_c, en_outputs[0])\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mde_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mde_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    897\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: call() takes 4 positional arguments but 5 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaWp85hHPacF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}